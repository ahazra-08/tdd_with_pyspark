{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad04037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"TDD - XXX\"). \\\n",
    "    config('spark.shuffle.useOldFetchProtocol', 'true'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe986acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read \\\n",
    ".format(\"text\") \\\n",
    ".load(\"/user/itv005880/project/tdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07d9a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Test-driven devel...|\n",
      "|                    |\n",
      "|Software engineer...|\n",
      "|                    |\n",
      "|Test-driven devel...|\n",
      "|                    |\n",
      "|Programmers also ...|\n",
      "|       1. Add a test|\n",
      "|The adding of a n...|\n",
      "|2. Run all tests....|\n",
      "|This shows that n...|\n",
      "|3. Write the simp...|\n",
      "|Inelegant or hard...|\n",
      "|4. All tests shou...|\n",
      "|If any fail, the ...|\n",
      "|5. Refactor as ne...|\n",
      "|Code is refactore...|\n",
      "|Examples of refac...|\n",
      "|moving code to wh...|\n",
      "|removing duplicat...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca65481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617d0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\\W+\". This splits the source at word boundaries removing spaces and punctuation.\n",
    "# explode: Returns a new row for each element in the given array or map\n",
    "def split_words_from_data(input_df):\n",
    "    return input_df.select(explode(split(input_df.value, \"\\W+\")).alias(\"word\"))\n",
    "\n",
    "words = split_words_from_data(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd90f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        word|\n",
      "+------------+\n",
      "|        Test|\n",
      "|      driven|\n",
      "| development|\n",
      "|         TDD|\n",
      "|          is|\n",
      "|           a|\n",
      "|    software|\n",
      "| development|\n",
      "|     process|\n",
      "|     relying|\n",
      "|          on|\n",
      "|    software|\n",
      "|requirements|\n",
      "|       being|\n",
      "|   converted|\n",
      "|          to|\n",
      "|        test|\n",
      "|       cases|\n",
      "|      before|\n",
      "|    software|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc6d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Test Cases Passed !\n"
     ]
    }
   ],
   "source": [
    "def test_split_words_from_data():\n",
    "    #Arrange\n",
    "    schema = StructType([\n",
    "        StructField(\"value\", StringType())\n",
    "    ])\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"word\", StringType())\n",
    "    ])\n",
    "    data = [\n",
    "        Row(\"TDD is a software\"),\n",
    "        Row(\"development process\"),\n",
    "    ]\n",
    "    expected_data = [\n",
    "        Row(\"TDD\"), Row(\"is\"), Row(\"a\"), Row(\"software\"), \n",
    "        Row(\"development\"), Row(\"process\")\n",
    "    ]\n",
    "    input_df = spark.createDataFrame(data, schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_schema)\n",
    "    \n",
    "    #Act\n",
    "    actual_df = split_words_from_data(input_df)\n",
    "    \n",
    "    #Assert\n",
    "    try:\n",
    "        assert actual_df.schema == expected_df.schema, \"Schema not matched\"\n",
    "        assert actual_df.collect() == expected_df.collect(), \"Data not matched\"\n",
    "        print(\"All Test Cases Passed !\")\n",
    "    except AssertionError as msg:\n",
    "        print(msg)    \n",
    "\n",
    "test_split_words_from_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84202ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower_case(words):\n",
    "    return words.select(lower(words.word).alias(\"word\"))\n",
    "\n",
    "lower_case_words = convert_to_lower_case(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc79f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Test Cases Passed !\n"
     ]
    }
   ],
   "source": [
    "def test_convert_to_lower_case():\n",
    "    #Arrange\n",
    "    schema = StructType([\n",
    "        StructField(\"word\", StringType())\n",
    "    ])\n",
    "    data = [\n",
    "        Row(\"Jack\"),\n",
    "        Row(\"aND\"),\n",
    "        Row(\"Jill\")\n",
    "    ]\n",
    "    expected_data = [\n",
    "        Row(\"jack\"), \n",
    "        Row(\"and\"),\n",
    "        Row(\"jill\")\n",
    "    ]\n",
    "    input_df = spark.createDataFrame(data, schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, schema)\n",
    "    \n",
    "    #Act\n",
    "    actual_df = convert_to_lower_case(input_df)\n",
    "    \n",
    "    #Assert\n",
    "    try:\n",
    "        assert actual_df.schema == expected_df.schema, \"Schema not matched\"\n",
    "        assert actual_df.collect() == expected_df.collect(), \"Data not matched\"\n",
    "        print(\"All Test Cases Passed !\")\n",
    "    except AssertionError as msg:\n",
    "        print(msg)    \n",
    "\n",
    "test_convert_to_lower_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42c91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        word|\n",
      "+------------+\n",
      "|        test|\n",
      "|      driven|\n",
      "| development|\n",
      "|         tdd|\n",
      "|          is|\n",
      "|           a|\n",
      "|    software|\n",
      "| development|\n",
      "|     process|\n",
      "|     relying|\n",
      "|          on|\n",
      "|    software|\n",
      "|requirements|\n",
      "|       being|\n",
      "|   converted|\n",
      "|          to|\n",
      "|        test|\n",
      "|       cases|\n",
      "|      before|\n",
      "|    software|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lower_case_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f88a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74b2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(lower_case_words):\n",
    "    return lower_case_words.groupBy(\"word\").count()\n",
    "    \n",
    "word_counts = word_count(lower_case_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3e1a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(word,StringType,true),StructField(count,LongType,false)))\n",
      "Schema not matched\n"
     ]
    }
   ],
   "source": [
    "def test_word_count():\n",
    "    #Arrange\n",
    "    schema = StructType([\n",
    "        StructField(\"word\", StringType())\n",
    "    ])\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"word\", StringType()),\n",
    "        StructField(\"count\", LongType())\n",
    "    ])\n",
    "    data = [\n",
    "        Row(\"jack\"),\n",
    "        Row(\"and\"),\n",
    "        Row(\"jac\"),\n",
    "        Row(\"jack\")\n",
    "    ]\n",
    "    expected_data = [\n",
    "        Row(\"jack\",2), \n",
    "        Row(\"and\",1),\n",
    "        Row(\"jac\",1) \n",
    "    ]\n",
    "    input_df = spark.createDataFrame(data, schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_schema)\n",
    "    \n",
    "    #Act\n",
    "    actual_df = word_count(input_df)\n",
    "    print(actual_df.schema)\n",
    "\n",
    "    #Assert\n",
    "    try:\n",
    "        assert actual_df.schema == expected_df.schema, \"Schema not matched\"\n",
    "        assert actual_df.collect() == expected_df.collect(), \"Data not matched\"\n",
    "        print(\"All Test Cases Passed !\")\n",
    "    except AssertionError as msg:\n",
    "        print(msg)\n",
    "    \n",
    "test_word_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "810adf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_sorted = word_counts.sort(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c55dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|  279|\n",
      "|     |  148|\n",
      "| test|  134|\n",
      "|    a|  131|\n",
      "|  and|  129|\n",
      "|   to|  122|\n",
      "|   of|  118|\n",
      "|tests|   90|\n",
      "|   is|   87|\n",
      "| that|   72|\n",
      "| code|   68|\n",
      "|   in|   68|\n",
      "|   be|   60|\n",
      "|  for|   54|\n",
      "|  tdd|   43|\n",
      "|  are|   39|\n",
      "|   or|   38|\n",
      "| this|   36|\n",
      "|   it|   36|\n",
      "| unit|   34|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word_counts_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c18480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
